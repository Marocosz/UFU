## 12/08/2024 ==================================================

## Regressão Linear Simples e Correlação

# Exercício 05
```{r}
x = c(20, 20, 20, 50, 50, 50, 80, 80, 80)

y = c(24.3, 24, 25.9, 16.5, 12.1, 13.7, 6.3, 5.7, 3.5)

# A)

reg = lm(y ~ x)
coef(reg)
confint(reg, level = 0.95)

# B)

predict(reg, data.frame(x = 30), level = 0.95, interval = c("confidence"))

# C)

predict(reg, data.frame(x = 30), level = 0.95, interval = c("prediction"))


```

# Exercício 06
```{r}

x = c(20, 20, 20, 50, 50, 50, 80, 80, 80)

y = c(24.3, 24, 25.9, 16.5, 12.1, 13.7, 6.3, 5.7, 3.5)

reg = lm(y ~ x)

# A)

confint(reg, level = 0.90)  # 28.683267 

# B)

confint(reg, level = 0.98)  # 34.594229

# C)

predict(reg, data.frame(x = 30), level = 0.96, interval = c("confidence"))

# D)

predict(reg, data.frame(x = 30), level = 0.99, inteval = c("prediction"))

```

# Exercício 07
```{r}
y = c(80, 70, 69, 69, 62, 61, 61, 54, 53, 52, 52, 47, 47, 47, 43, 40, 38, 37, 34, 32)

x = c(29, 19, 12, 18, 13, 19, 12, 1, 7, -1, -10, -2, -10, -3, -5, -25, -17, -12, -17, -28)

# A)

cor(x, y)  # r = 0.9414  -  Correlação 

# B)
# H0: rho = 0
# H1: rho != 0

cor.test(x, y, alternative=c("two.sided"), conf.level = 0.95)
# Conclusão: Rejeita-se H0, ou seja, há evidencias que PG e SG são correlacionados

# C)

reg = lm(y ~ x)
# summary(reg)

# D)
# H0: beta1 = 0
# H1: beta1 != 0


```

## Aula 14/08/24 ==============================================



## Regressão Linear Múltipla - Parte 1

LETRAS GREGAS == POPULAÇÃO
LETRAS DO ALFABETO == AMOSTRA

# Exercício 01
```{r}
y = c(9, 10, 9, 12, 11, 10, 10, 16, 7, 12, 11, 15, 10, 13, 8, 14)

x1 = c(50.8, 76.2, 50.8, 76.2, 50.8, 76.2, 50.8, 76.2, 50.8, 76.2, 50.8, 76.2, 50.8, 76.2, 50.8, 76.2)

x2 = c(15, 15, 25, 25, 15, 15, 25, 25, 15, 15, 25, 25, 15, 15, 25, 25)

x3 = c(0.635, 0.635, 0.635, 0.635, 1.016, 1.016, 1.016, 1.016, 0.635, 0.635, 0.635, 0.635, 1.016, 1.016, 1.016, 1.016)

reg = lm(y ~ x1 + x2 + x3)

# A)

summary(reg)  # Pegaremos os dados "Estimate" de x1 x2 e x3 onde "intercept" = Y


# B)

summary(reg)$sigma  # == sigma com ^
#ou
sqrt(deviance(reg)/df.residual(reg))

# C)

summary(reg)$coef
#ou
sqrt(diag(vcov(reg)))

# D)

summary(reg)$r.sq

```

## Aula 19/08 ==============================================================

# Exercício 02
```{r}
dados = data.frame("Pts" = c(80, 70, 69, 69, 62, 61, 61, 54, 53, 52, 52, 47, 47, 47, 43, 40, 38,  37, 34, 32),
                   "GP" = c(67, 59, 53, 49, 51, 61, 36, 43, 42, 46, 36, 38, 37, 42, 39, 34, 37, 31, 31, 28),
                   "GC" = c(38, 40, 41, 31, 38, 42, 24, 42, 35, 47, 46, 40, 47, 45, 44, 59, 54, 43, 48, 56))


# A)
# Aleatórias


# B)

# Como os dados estão em um dataframe, temos que usar (data = data|_frame)
reg = lm(Pts ~ GP + GC, data = dados)
coef(reg)
summary(reg)
# Ûpts = Beta0 + beta1 * Gp + Beta2 * GC = 40,02 + 0,911*GP - 0,6232*GC


# C)

summary(reg)$sigma # 4.51215
#ou
sqrt(deviance(reg)/df.residual(reg)) # 4.51215


# D)

summary(reg)
#ou
summary(reg)$coef
#ou
sqrt(diag(vcov(reg)))


# E)
summary(reg)$r.sq  # R² = 0,8980
summary(reg)$adj.r.sq  # R²aj = 0,8861
```

## Regressão Linear Múltipla - Parte 2

# Exercício 01
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)
summary(reg)
# valor-p = 7,249x10^-15
# Conclusão: rejeita-se H0, ou seja, há evidências que pelo menos um dos betas é diferente de zero (a regressão é a adequada)
```


# Exercício 02
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)


# A)

# H0: Beta1 = 0
# H1: Beta1 != 0

summary(reg)$coef
t0 = (-0.02860886)/0.09060154
t0  # -0.3157657
2*pt(abs(t0), 27,lower=FALSE) # 0.7546083
# Conclusão: falha-se em rejeitar H0, ou seja, não há evidências que beta1 != 0


# B)

# H0: Beta1 = 0
# H1: Beta1 < 0

t0 = (-4.32005167)/2.85096732 
t0 # -1.515293
pt(t0, 27) # 0.07065943
# Conclusão: Falha-se em rejeitar H0, ou seja, não há evidências que Beta3 < 0

# C)

# H0: Beta4 = 4
# H1: Beta4 > 4

t0 = (8.97488928 - 4)/2.77263148
t0 # 1.794284
pt(t0, 27, lower=FALSE) # 0.04198695
# Conclusão: Rejeita-se H0, ou seja, há evidências que Beta4 > 4
```

## Aula 21/08 ================================================

# Exercício 03
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)

# A)
# 1 - alpha = 0,99

confint(reg, level=0.99)
# -0,2796 <= Beta1 <= 0,2224


# B)
# Fazer gráficos
confint(reg, level = 0.98)
# beta1 >= -0,2526
```

# Exercício 04
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)

# A)
# p = 3  alpha = 0,005  alpha/3 (ajuste de Bonferroni)
confint(reg, level = (1 - 0.05/3))

# {-0,2599 <= beta1 <= 0,2026} interseção {0,0430 <= beta2 <= 0,3887} interseção {-11,5970 <= beta3 <= 2,9569 }


# B)
# p = 5
# Fazer os gráficos
confint(reg, level = 0.98)
# {beta0 <= 5,7659} interseção {beta <= 0,1954} interseção {...} até o beta4


# C)
# p = 2
# Fazer os gráficos
confint(reg, level = 0.99)
# {beta1 >= -0,2796} interseção {beta2 >= 0,0282}


```

# Exercício 05
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)

# A)
# 1 - alpha = 0,95
# p = 2
# n = length(y) = quantidade de dadaos da amostra
n = length(y) #  = 32
# n - p = 30
# beta1(chapeuzin) - ep(chapeuzin)(beta1(chapeuzin)) * sqrt(2*qf(0,95, 2, 30)) <= Beta1(chapeuzin) <= ...>
# beta1(chapeuzin) + ep(chapeuzin)(beta1(chapeuzin))

# mesma coisa pro beta2
summary(reg)$coef
# beta(chapeuzin) = coluna do "Estimate" e ep(chapeuzin) = coluna do "std. error"

# Beta1
summary(reg)$coef[2,1] - summary(reg)$coef[2,2]*sqrt(2*qf(0.95, 2, 30)) # -0.261926
summary(reg)$coef[2,1] + summary(reg)$coef[2,2]*sqrt(2*qf(0.95, 2, 30)) # 0.2047083

# Beta2
summary(reg)$coef[3,1] - summary(reg)$coef[3,2]*sqrt(2*qf(0.95, 2, 30)) # 0.04142947
summary(reg)$coef[3,1] + summary(reg)$coef[3,2]*sqrt(2*qf(0.95, 2, 30)) # 0.3902044

# {-0.261926 <= beta1 <= # 0.2047083} interseção {0.04142947 <= beta2 <= 0.3902044}
```

# Exercício 06
```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 4.60, 4.30, 4.53, 4.02, 2.90,4.02,4.40, 3.98, 4.31, 4.39, 4.27,  2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)

sat = reg

red = lm(y ~ x2 + x4) # Reduzido

# H0: Beta1 = Beta3 = 0
# H1: Betai != 0  (pelo menos)

anova(red, sat)  # f0 = 2,4929 e a P(x >= F0) = 0,1015

# Conclusão: Falha-se em rejeitar H0, ou seja, não há evidências que o modelo reduzido seja pior
# modelo reduzido deve ser adotado

```

## Aula 26/08 =====================================================================

# Regressão Linear Multipla - Parte 3


# Exercício 01
```{r}
y = c(271.8, 264.0, 238.8, 230.7, 251.6, 257.9, 263.9, 266.5, 229.1, 239.3, 258.0, 257.6, 267.3, 
267.0, 259.6, 240.4, 227.2, 196.0, 278.7, 272.3, 267.4, 254.5, 224.7, 181.5, 227.5, 253.6, 
263.0, 265.8, 263.8)

x1 = c(40.55, 36.19, 37.31, 32.52, 33.71, 34.14, 34.85, 35.89, 33.53, 33.79, 34.72, 35.22, 
36.50, 37.60, 37.89, 37.71, 37.00, 36.76, 34.62, 35.40, 35.96, 36.26, 36.34, 35.90, 31.84, 
33.16, 33.83, 34.89, 36.27)
 
x2 = c(16.66, 16.46, 17.66, 17.50, 16.40, 16.28, 16.06, 15.93, 16.60, 16.41, 16.17, 15.92, 
16.04, 16.19, 16.62, 17.37, 18.12, 18.53, 15.54, 15.70, 16.45, 17.62, 18.12, 19.05, 16.51, 
16.02, 15.89, 15.83, 16.71)
 
# alpha = 0,01
reg = lm(y ~ x1 + x2)

# A)

# ^sigma^2 = ^alpha0 + ^alpha1 * ^miy

# H0: alpha1 = 0
# H1: alpha1 != 0

e2 = residuals(reg)^2
aux = lm(e2 ~ fitted(reg))
n = length(e2)  # 29
R2 = summary(aux)$r.sq  # 0.2631809
LM = n*R2  # 7.632246
valorp = pchisq(LM, 1, lower=FALSE)  # 0.005733387

e2
aux
n
R2
LM
valorp
df.residual(aux)

# Conclusão: 0.005733387 < alpha: Rejeita-se H0, ou seja, há evidências que a variância seja linearmente
# relacionada com os valores ajustados


# B)

# ^sigma^2 = ^alpha0 + ^alpha1 * ^miy + ^alpha2 * ^miy^2

# H0: alpha1 = alpha2 = 0
# H1: alphai != 0

# GL = 2

e2 = residuals(reg)^2
aux = lm(e2 ~ fitted(reg) + I(fitted(reg)^2))
n = length(e2)  # 29
R2 = summary(aux)$r.sq  # 0.2631809
LM = n * R2  # 7.72772
valorp = pchisq(LM, 2, lower=FALSE)  # 0.02098683

e2
aux
n
R2
LM
valorp

# Conclusão: 0.02098683 > alpha: Falha-se em rejeitar H0, ou seja, não há evidências que a variância seja uma função 
# polinomial de grau 2 nos valores ajustados
 
```

# Exercício 02

```{r}
y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 
34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 
60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 
62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 
4.60, 4.30, 4.53, 4.02, 2.90, 4.02, 4.40, 3.98, 4.31, 4.39, 4.27, 2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 
4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

reg = lm(y ~ x1 + x2 + x3 + x4)
# ^sigma^2 = ^alpha0 + ^alpha1 * x1 + ^alpha2 * x3 + ^alpha3 * x4

# H0: alpha1 = alpha2 = alpha3 = 0
# H1: alphai != 0

e2 = residuals(reg)^2
aux = lm(e2 ~ x1 + x3 + x4)
n = length(e2)  # 32
R2 = summary(aux)$r.sq # 0.3222501
LM = n * R2  # 10.312
# P(x >= LM)
valorp = pchisq(LM, 3, lower=FALSE)  # 0.01609195

e2
aux
n
R2
LM
valorp

# Conclusão: rejeita-se H0, ou seja, há evidências que a variância é função çinear de x1, x3 e x4


```


# Exercício 03

```{r}
y = c(271.8, 264.0, 238.8, 230.7, 251.6, 257.9, 263.9, 266.5, 229.1, 239.3, 258.0, 257.6, 267.3, 
267.0, 259.6, 240.4, 227.2, 196.0, 278.7, 272.3, 267.4, 254.5, 224.7, 181.5, 227.5, 253.6, 
263.0, 265.8, 263.8)

x1 = c(40.55, 36.19, 37.31, 32.52, 33.71, 34.14, 34.85, 35.89, 33.53, 33.79, 34.72, 35.22, 
36.50, 37.60, 37.89, 37.71, 37.00, 36.76, 34.62, 35.40, 35.96, 36.26, 36.34, 35.90, 31.84, 
33.16, 33.83, 34.89, 36.27)
 
x2 = c(16.66, 16.46, 17.66, 17.50, 16.40, 16.28, 16.06, 15.93, 16.60, 16.41, 16.17, 15.92, 
16.04, 16.19, 16.62, 17.37, 18.12, 18.53, 15.54, 15.70, 16.45, 17.62, 18.12, 19.05, 16.51, 
16.02, 15.89, 15.83, 16.71)

reg = lm(y ~ x1 + x2)

# H0: ri é normal
# H1: ri não é normal

# alpha = 0,05

shapiro.test(rstandard(reg))
valorp = 0.5507

# Conclusão: Falha-se em rejeitar H0, ou seja, não evidências que os resíduos padronizados não sejam normais.

plot(reg)


```


# Exercício 04
```{r}
y = c(271.8, 264.0, 238.8, 230.7, 251.6, 257.9, 263.9, 266.5, 229.1, 239.3, 258.0, 257.6, 267.3, 
267.0, 259.6, 240.4, 227.2, 196.0, 278.7, 272.3, 267.4, 254.5, 224.7, 181.5, 227.5, 253.6, 
263.0, 265.8, 263.8)

x1 = c(40.55, 36.19, 37.31, 32.52, 33.71, 34.14, 34.85, 35.89, 33.53, 33.79, 34.72, 35.22, 
36.50, 37.60, 37.89, 37.71, 37.00, 36.76, 34.62, 35.40, 35.96, 36.26, 36.34, 35.90, 31.84, 
33.16, 33.83, 34.89, 36.27)
 
x2 = c(16.66, 16.46, 17.66, 17.50, 16.40, 16.28, 16.06, 15.93, 16.60, 16.41, 16.17, 15.92, 
16.04, 16.19, 16.62, 17.37, 18.12, 18.53, 15.54, 15.70, 16.45, 17.62, 18.12, 19.05, 16.51, 
16.02, 15.89, 15.83, 16.71)

# Não precisamos do Y para multicolinearidade

# A)

X = cbind(x1, x2)
diag(solve(cor(X)))

# FIV(^Beta1) = 1,090043 < 4
# FIV(^Beta2) = 1,090043 < 4 (o numero 4 é definido por "convenção" como maior erro padrão permitido)


# B)

y = c(29, 40, 24, 46, 26, 55, 22, 52, 27, 29, 21, 22, 33, 31, 34, 45, 32, 37, 34, 37, 20, 33, 36, 27, 34, 
34, 23, 19, 24, 16, 32, 22)

x1 = c(33, 90, 31, 90, 33, 92, 37, 91, 36, 61, 35, 59, 59, 88, 60, 91, 59, 63, 60, 60, 34, 60, 60, 59, 
60, 59, 60, 37, 62, 35, 62, 37)

x2 = c(53, 64, 36, 60, 51, 92, 51, 92, 54, 62, 35, 42, 56, 65, 60, 89, 60, 62, 60, 61, 35, 62, 59, 62, 
62, 62, 36, 35, 38, 35, 61, 37)

x3 = c(3.32, 7.32, 3.10, 7.32, 3.18, 7.45, 3.39, 7.27, 3.20, 3.91, 3.03, 3.75, 4.78, 6.48, 4.72, 6.70, 
4.60, 4.30, 4.53, 4.02, 2.90, 4.02, 4.40, 3.98, 4.31, 4.39, 4.27, 2.75, 4.41, 2.59, 4.39, 2.73)

x4 = c(3.42, 6.70, 3.26, 7.20, 3.18, 7.45, 3.08, 7.26, 3.41, 4.08, 3.03, 3.45, 4.57, 5.80, 4.72, 6.60, 
4.41, 4.30, 4.53, 4.10, 2.95, 3.89, 4.36, 4.02, 4.42, 4.53, 3.94, 2.64, 3.49, 2.59, 4.39, 2.59)

X = cbind(x1, x2, x3, x4)
diag(solve(cor(X)))

# FIV(^Beta1) = 12.997379 > 4
# FIV(^Beta2) = 4.720998 > 4
# FIV(^Beta3) = 71.301491 > 4
# FIV(^Beta4) = 61.932647 > 4

# Como há maiores que quatro (todos) há problemas de multicolinearidades
```

## Aula 02/09 =================================================================


# Exercício 05
```{r}
y = c(271.8, 264.0, 238.8, 230.7, 251.6, 257.9, 263.9, 266.5, 229.1, 239.3, 258.0, 257.6, 267.3, 
267.0, 259.6, 240.4, 227.2, 196.0, 278.7, 272.3, 267.4, 254.5, 224.7, 181.5, 227.5, 253.6, 
263.0, 265.8, 263.8)

x1 = c(40.55, 36.19, 37.31, 32.52, 33.71, 34.14, 34.85, 35.89, 33.53, 33.79, 34.72, 35.22, 
36.50, 37.60, 37.89, 37.71, 37.00, 36.76, 34.62, 35.40, 35.96, 36.26, 36.34, 35.90, 31.84, 
33.16, 33.83, 34.89, 36.27)
 
x2 = c(16.66, 16.46, 17.66, 17.50, 16.40, 16.28, 16.06, 15.93, 16.60, 16.41, 16.17, 15.92, 
16.04, 16.19, 16.62, 17.37, 18.12, 18.53, 15.54, 15.70, 16.45, 17.62, 18.12, 19.05, 16.51, 
16.02, 15.89, 15.83, 16.71)

reg = lm(y ~ x1 + x2)
rstudent((reg))
length(rstudent(reg)) # n = 29
alpha = 0.05
gl = 25  # n - K - 2 = 29 - 2 - 2

qt(alpha/58, 25, lower=FALSE)  # 3.509449
qt(alpha/58, 25)  # -3.509449

# Usamos esse 58 ao utilizar o metodo de bon ferroni multiplicando 2*29 para evitar alguns problemas

sort(rstudent(reg))
maior_rti = -1.9932  # > -3.509
menor_rti = 3.1860  # < 3.509  ou seja, pelos maiores e menores dados estarem dentro do intervalo dos qt

# H0: Nenhum outlier
# H1: Pelo menos um outlier

# Conclusão, falha-se em rejeitar H0, ou seja, não há evidências que algum rti seja outlier

(rstudent(reg) > -3.509) & (rstudent(reg) < 3.509)
```








